<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Activation Functions - GERO AI</title>
    </head>
    <body>
        <div class="header">
            <h1> - GERO AI - Neural Network Types</h1>
            <button><a href="../">Back to Neural Network Main Documentation</a></button>
        </div>
        <h1>Activation Functions</h1>
        <p>
            Activation functions are a crucial component of neural networks, determining the output of a neuron given an input or set of inputs. They introduce non-linearity into the model, allowing it to learn complex patterns in the data. Common activation functions include:
            <ul>
                <li><strong>Sigmoid:</strong> Outputs values between 0 and 1, useful for binary classification.</li>
                <li><strong>Tanh:</strong> Outputs values between -1 and 1, often used in hidden layers.</li>
                <li><strong>ReLU (Rectified Linear Unit):</strong> Outputs the input directly if positive; otherwise, it outputs zero. It is widely used due to its simplicity and efficiency.</li>
                <li><strong>Leaky ReLU:</strong> A variant of ReLU that allows a small, non-zero gradient when the unit is not active.</li>
                <li><strong>Softmax:</strong> Used in the output layer for multi-class classification problems, converting logits into probabilities.</li>
        </p>

        <label>In Summary:</label>
        <table>
            <thead>
                <tr>
                    <th>Activation Function</th>
                    <th>Output Range</th>
                    <th>Use Case</th>
                    <th>Equation</th>
                    <th>JQuery</th>
                </tr>
            </thead>

            <tbody>
                <tr>
                    <td>Sigmoid</td>
                    <td>(0,1)</td>
                    <td>Binary classification tasks<br>Probability<br>Yes/No</td>
                    <td>σ(x) = 1 / (1 + e^(-x))</td>
                    <td>
                        <pre>
                            <code>
                                function sigmoid(x)
                                {
                                    return 1 / (1 + Math.exp(-x));
                                }
                            </code>
                        </pre>
                    </td>
                </tr>
                <tr>
                    <td>Tanh</td>
                    <td>(-1,1)</td>
                    <td>Hidden layers in neural networks.(Older Nets)</td>
                    <td>tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))</td>
                    <td>
                        <pre>
                            <code>
                                function tanh(x)
                                {
                                    return (Math.exp(x) - Math.exp(-x)) / (Math.exp(x) + Math.exp(-x));
                                }
                            </code>
                        </pre>
                    </td>
                </tr>
                <tr>
                    <td>ReLU</td>
                    <td>[0, ∞)</td>
                    <td>Hidden layers in neural networks.(Modern Nets)</td>
                    <td>ReLU(x) = max(0, x)</td>
                    <td>
                        <pre>
                            <code>
                                function relu(x)
                                {
                                    return Math.max(0, x);
                                }
                            </code>
                        </pre>
                    </td>
                </tr>
                <tr>
                    <td>Leaky ReLU</td>
                    <td>(-∞, ∞)</td>
                    <td>Hidden layers. Addresses the "dying ReLU" problem.</td>
                    <td>Leaky ReLU(x) = max(αx, x)</td>
                    <td>
                        <pre>
                            <code>
                                function leakyRelu(x, alpha = 0.01)
                                {
                                    return Math.max(alpha * x, x);
                                }
                            </code>
                        </pre>
                    </td>
                </tr>
                <tr>
                    <td>Parameter ReLU</td>
                    <td>(-∞, +∞)</td>
                    <td>Hidden layers. Used in deep CNN. Overkill for small games/simple bots</td>
                    <td>PReLU(x) = max(αx, x)</td>
                    <td>
                        <pre>
                            <code>
                                function prelu(x, alpha) {
                                    return (x >= 0) ? x : alpha * x;
                                }
                            </code>
                        </pre>
                    </td>
                </tr>
                <tr>
                    <td>Softmax</td>
                    <td>(0,1)</td>
                    <td>Multi-class classification output layer.</td>
                    <td>softmax(x_i) = e^(x_i) / Σ(e^(x_j))</td>
                    <td>
                        <pre>
                            <code>
                                function softmax(array) {
                                    // Calculate e^x_i for each element
                                    const expValues = array.map(x => Math.exp(x));
                                    
                                    // Calculate sum of all e^x_j
                                    const sumExp = expValues.reduce((sum, val) => sum + val, 0);
                                    
                                    // Divide each e^x_i by the sum
                                    return expValues.map(val => val / sumExp);
                                }
                            </code>
                        </pre>
                    </td>
                </tr>
            </tbody>
        </table>
    </body>
</html>